{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MLP model on insurance claims data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (1,2)\n",
    "dims = (2,3)\n",
    "\n",
    "for (idx,i) in enumerate(indices):\n",
    "    if i not in list(range(dims[idx])):\n",
    "        print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer init seed: 6823321540184574520\n",
      "dataset seed: 3018045280055472080\n",
      "/home/ola/Code/dydx/dydx/data\n",
      "Number of parameters: 47777\n",
      "{'Agency': 16, 'Agency Type': 2, 'Destination': 102, 'Distribution Channel': 2, 'Gender': 3, 'Product Name': 25}\n",
      "[('Age', 'ordinal'),\n",
      " ('Agency', 'categorical'),\n",
      " ('Agency Type', 'binary'),\n",
      " ('Commision (in value)', 'ordinal'),\n",
      " ('Destination', 'categorical'),\n",
      " ('Distribution Channel', 'binary'),\n",
      " ('Duration', 'ordinal'),\n",
      " ('Gender', 'cateogrical'),\n",
      " ('Net Sales', 'ordinal'),\n",
      " ('Product Name', 'categorical')]\n",
      "{'Agency': 16, 'Destination': 102, 'Gender': 3, 'Product Name': 25}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ln': dict_keys(['e0', 'e1', 'e2', 'e3', 'l1', 'l2', 'l3', 'l4', 'l5']),\n",
       " 'e0': <dydx.layers.Embedding at 0x790a875a83b0>,\n",
       " 'e1': <dydx.layers.Embedding at 0x790a701444d0>,\n",
       " 'e2': <dydx.layers.Embedding at 0x790a661e9220>,\n",
       " 'e3': <dydx.layers.Embedding at 0x790a661eb6e0>,\n",
       " 'l1': <dydx.layers.Linear at 0x790a875a89e0>,\n",
       " 'l2': <dydx.layers.Linear at 0x790a67ef2840>,\n",
       " 'l3': <dydx.layers.Linear at 0x790a67513140>,\n",
       " 'l4': <dydx.layers.Linear at 0x790a6713ad80>,\n",
       " 'l5': <dydx.layers.Linear at 0x790a671d4800>}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dydx.layers import Linear, Embedding\n",
    "from dydx.model import Model\n",
    "from dydx.dataset import xcols_types,xcols\n",
    "from dydx.download import encodings_dims, encodings_dims\n",
    "\n",
    "\n",
    "SCALER = 2\n",
    "EMBEDDING_DIM = 32\n",
    "cat_data_size  = {k:v for (k,v) in encodings_dims.items() if v > 2 }\n",
    "\n",
    "embedders =[ (f'e{idx}',Embedding(dims=(in_size, EMBEDDING_DIM))) for (idx,in_size) in  enumerate(cat_data_size.values())]\n",
    "\n",
    "\n",
    "l1 = Linear( random=True, dims=(4*EMBEDDING_DIM + 6, SCALER*64))\n",
    "\n",
    "l2 = Linear( random=True,dims=(SCALER*64, SCALER*64))\n",
    "l3 = Linear( random=True, dims=(SCALER*64, SCALER*32))\n",
    "l4 = Linear( random=True,dims=(SCALER*32, 16))\n",
    "l5 = Linear( activation=False, random=True, dims=(16,1)\n",
    ") # when activation is set to false we use sigmoid function to normalised output \n",
    "\n",
    "\t\t\n",
    "names = [\"l1\",\"l2\",\"l3\",\"l4\", \"l5\"]\t\t\n",
    "layers =[l1,l2,l3,l4, l5]\n",
    "ls = {**dict(embedders), **dict(zip(names,layers))}\n",
    "model_fc1 = Model(dict(zip(names,layers)))\n",
    "model = Model(ls)\n",
    "print(\"Number of parameters:\", len(model.parameters()))\n",
    "\n",
    "print(encodings_dims)\n",
    "from pprint import pprint \n",
    "pprint(list(zip(xcols,xcols_types)))\n",
    "print({k:v for (k,v) in encodings_dims.items() if v > 2 })\n",
    "# print(xcols)\n",
    "model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sizes for train, val, test are [17441, 4983, 2492] respectively.\n"
     ]
    }
   ],
   "source": [
    "from dydx.metrics import Loss\n",
    "from dydx.dataset import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "loss = Loss()\n",
    "EPOCHS= 1000\n",
    "\n",
    "\n",
    "ds = Dataset(testing=False) # takes 4048 examples to overfit model too for testing purposes\n",
    "fname = f\"{list(set(model.seed()))[0]}_{ds.seed}.data\"\n",
    "fp =  Path().cwd() / \"experiments\" / fname\n",
    "splits = ['train','val','test']\n",
    "print(f\"dataset sizes for {', '.join(splits)} are {[ds.__len__(split) for split in splits]} respectively.\")\n",
    "dst = ds.train\n",
    "dsv = ds.val\n",
    "dss = ds.test\n",
    "# iterators \n",
    "dltrain  = DataLoader(dst,32)\n",
    "dlval = DataLoader(dsv,32)\n",
    "dltest = DataLoader(dss,23)\n",
    "for (idx,xy) in enumerate( dltrain()):\n",
    "    x,y = xy\n",
    "    if idx == 0:\n",
    "        pass\n",
    "        # Agency\n",
    "        # a_in = Array([x.T().values[:][1]]).T()\n",
    "        # x0 = model.e0(a_in)\n",
    "        # # Destination\n",
    "        # d_in = Array([x.T().values[:][4]]).T()\n",
    "        # x1 = model.e1(d_in)\n",
    "        # # Gender\n",
    "        # g_in = Array([x.T().values[:][7]]).T()\n",
    "        # x2 = model.e2(g_in)\n",
    "        # # Product Name\n",
    "        # p_in = Array([x.T().values[:][9]]).T()\n",
    "        # x3 = model.e3(p_in)\n",
    "        \n",
    "        # x4 = Array([x.T().values[:][0]]).T()\n",
    "        # x5 = Array([x.T().values[:][2]]).T()\n",
    "        # x6 = Array([x.T().values[:][3]]).T()\n",
    "        # x7 = Array([x.T().values[:][5]]).T()\n",
    "        # x8 = Array([x.T().values[:][6]]).T()\n",
    "        # x9 = Array([x.T().values[:][8]]).T()\n",
    "        \n",
    "        \n",
    "        # features = [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9]\n",
    "        # stack = lambda x, y : x.stack(y,dim=0)\n",
    "        # x = reduce(stack, features)\n",
    "        # x = model.l1(x)\n",
    "        # x = model.l2(x)\n",
    "        # x = model.l3(x)\n",
    "        # x = model.l4(x)\n",
    "        # y_hat = model.l5(x)\n",
    "        \n",
    "        \n",
    "def propagate(x,model):    \n",
    "    a_in = x[:,1]\n",
    "    x0 = model.e0(a_in)\n",
    "    # Destination\n",
    "    d_in = x[:,4]\n",
    "    x1 = model.e1(d_in)\n",
    "    # Gender\n",
    "    g_in = x[:,7]\n",
    "    x2 = model.e2(g_in)\n",
    "    # Product Name\n",
    "    p_in = x[:,9]\n",
    "    x3 = model.e3(p_in)\n",
    "    x4 = x[:,0]\n",
    "    x5 = x[:,2]\n",
    "    x6 = x[:,3]\n",
    "    x7 = x[:,5]\n",
    "    x8 = x[:,6]\n",
    "    x9 = x[:,8]\n",
    "    features = [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9]\n",
    "    stack = lambda x, y : x.stack(y,dim=0)\n",
    "    x = reduce(stack, features)\n",
    "    x = model.l1(x)\n",
    "    x = model.l2(x)\n",
    "    x = model.l3(x)\n",
    "    x = model.l4(x)\n",
    "    y_hat = model.l5(x)\n",
    "    \n",
    "    return y_hat, model\n",
    "\n",
    "\n",
    "\n",
    "def forward(self,x):    \n",
    "    a_in = x[:,1]\n",
    "    x0 = self.e0(a_in)\n",
    "    # Destination\n",
    "    d_in = x[:,4]\n",
    "    x1 = self.e1(d_in)\n",
    "    # Gender\n",
    "    g_in = x[:,7]\n",
    "    x2 = self.e2(g_in)\n",
    "    # Product Name\n",
    "    p_in = x[:,9]\n",
    "    x3 = self.e3(p_in)\n",
    "    x4 = x[:,0]\n",
    "    x5 = x[:,2]\n",
    "    x6 = x[:,3]\n",
    "    x7 = x[:,5]\n",
    "    x8 = x[:,6]\n",
    "    x9 = x[:,8]\n",
    "    features = [x0, x1, x2, x3, x4, x5, x6, x7, x8, x9]\n",
    "    stack = lambda x, y : x.stack(y,dim=0)\n",
    "    x = reduce(stack, features)\n",
    "    x = self.l1(x)\n",
    "    x = self.l2(x)\n",
    "    x = self.l3(x)\n",
    "    x = self.l4(x)\n",
    "    y_hat = self.l5(x)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "# model.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING = None\n",
    "losses = []\n",
    "OPTMISATION = ['sgd', 'sgd+m'][1]\n",
    "fname = f\"{OPTMISATION}_{list(set(model.seed()))[0]}_{ds.seed}.data\"\n",
    "fp =  Path().cwd() / \"experiments\" / fname\n",
    "#print(f'Number of parameterers: {len(model.parameters().parameters)}')\n",
    "\n",
    "delta_t_minus1 = [0] *  len(model.parameters())\n",
    "step = 0 \n",
    "for epoch in range(EPOCHS):\n",
    "    # print(\"Training\".center(70,\"#\"))\n",
    "    for (idx,xy) in enumerate( dltrain()):\n",
    "        x,y = xy\n",
    "        \n",
    "        # y_hat =model(x)\t\t\n",
    "        y_hat, model = propagate(x,model)\n",
    "        acc, avg_loss = loss(y,y_hat)\n",
    "\n",
    "        avg_loss.backward()\n",
    "\n",
    "        losses.append(avg_loss.data)\n",
    "        info  = f'Step {step} Loss:{avg_loss.data} train accuracy:{acc*100}%'\n",
    "        with open(fp, \"a+\") as f:\n",
    "            f.write(info + '\\n')\n",
    "  \n",
    "        print(info)\n",
    "        \n",
    "        # optimizer step\n",
    "        LR = 0.0000001\n",
    "        if OPTMISATION == 'sgd+m':\n",
    "            # SGD with momentum\n",
    "            BETA = 0.9\n",
    "            for idx, p in enumerate(model.parameters()):\n",
    "                # if idx in [1,2,3,4]:\n",
    "                # \tprint('p.data:', p.data,  'p.grad:',p.grad)\n",
    "                p.data = p.data -  LR *( BETA*p.grad + (1-BETA)*delta_t_minus1[idx])\n",
    "                delta_t_minus1[idx] =  p.grad + (1-BETA)*delta_t_minus1[idx]\n",
    "        if OPTMISATION == 'sgd':\n",
    "            # SGD \n",
    "            for idx, p in enumerate(model.parameters()):\n",
    "                # if idx in [1,2,3,4]:\n",
    "                # \tprint('p.data:', p.data,  'p.grad:',p.grad)\n",
    "                p.data = p.data -  LR *p.grad\n",
    "        model.zero_grad()\n",
    "        step += 1 \n",
    "\n",
    "        # print(\"After zero-ing grad\")\n",
    "        # for idx,p in enumerate(model.parameters()):\n",
    "        # \tif idx in [1,2,3,4]:\n",
    "        # \t\tprint('p.grad:',p.grad)\t\t\t      \n",
    "        # # checking if gradients were actually set to zero \n",
    "        # for p in model.parameters():\n",
    "        # \tprint('p.grad:',p.grad)\n",
    "\n",
    "# \tprint(\"Validation\".center(70,\"#\"))\n",
    "# \tfor (idx,xy) in enumerate( dlval()):\n",
    "# \t\tx,y = xy \n",
    "# #\t\t\tfor layer in model.layers:\n",
    "# #\t\t\t\tx = layer(x)\n",
    "# \t\ty_hat = model(x)\n",
    "# \t\tacc, avg_loss= loss(y,y_hat)\n",
    "# \t\tprint(f'Epoch {epoch}:{idx} Loss:{avg_loss.data} val accuracy:{acc*100}%')\t\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dydx.metrics import Loss\n",
    "from dydx.linear_algebra import Array\n",
    "from dydx.layers import Linear\n",
    "from dydx.dydx import Scalar\n",
    "from dydx.model import Model\n",
    "loss = Loss()\n",
    "\n",
    "# layer seed: 8433248431303025905\n",
    "# dataset seed: 3588574048688484214\n",
    "X = Array([[Scalar(0.),Scalar(0.)],\n",
    "           [Scalar(0.),Scalar(1.)],\n",
    "           [Scalar(1.),Scalar(0.)],\n",
    "           [Scalar(1.),Scalar(1.)]])\n",
    "\n",
    "y  = Array([[Scalar(0.)],\n",
    "            [Scalar(1.)], \n",
    "            [Scalar(1.)],\n",
    "            [Scalar(0.)]])\n",
    "\n",
    "l1 = Linear( random=True,dims=(2, 16))\n",
    "l2 = Linear( random=True, dims=(16, 16))\n",
    "l3 = Linear( random=True,dims=(16, 1))\n",
    "layers = ['l1', 'l2', 'l3']\n",
    "model = Model(dict(zip(layers,[l1,l2,l3])))\n",
    "\n",
    "losses= []\n",
    "EPOCHS = 250_000_0000\n",
    "for epoch in range(EPOCHS):\n",
    "\t# print(\"Training\".center(70,\"#\"))\n",
    "\ty_hat = model(X)\n",
    "\tacc, avg_loss = loss(y,y_hat)\n",
    "\n",
    "\tavg_loss.backward()\n",
    "\n",
    "\tlosses.append(avg_loss.data)\n",
    "\t\n",
    "\tinfo  = f'Epoch {epoch} Loss:{avg_loss.data} train accuracy:{acc*100}%'\n",
    "\tprint(info)\n",
    "\n",
    "\tlr = 0.0000001\n",
    "\tfor idx, p in enumerate(model.parameters()):\n",
    "\t\t# if idx in [1,2,3,4]:\n",
    "\t\t# print('p.data:', p.data,  'p.grad:',p.grad)\n",
    "\t\tp.data = p.data -  lr *p.grad\n",
    "  \n",
    "  \n",
    "\tmodel.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch-equivalent model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dydx.dataset import DataLoader, Dataset\n",
    "ds = Dataset(testing=True) # takes 160 examples to overfit model too for testing purposes\n",
    "splits = ['train','val','test']\n",
    "print(f\"dataset sizes for {', '.join(splits)} are {[ds.__len__(split) for split in splits]} respectively.\")\n",
    "dst = ds.train\n",
    "dsv = ds.val\n",
    "dss = ds.test\n",
    "# iterators \n",
    "dltrain  = DataLoader(dst,32)\n",
    "dlval = DataLoader(dsv,32)\n",
    "dltest = DataLoader(dss,23)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class ClaimsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "\n",
    "model = ClaimsClassifier()\n",
    "\n",
    "for _ in range(100000):\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    for xy in dltrain():\n",
    "        x, y = xy\n",
    "        x = Tensor([[x_.data for x_ in row] for row in x.values])\n",
    "        y = Tensor([y_.data for y_ in y.values])\n",
    "        \n",
    "        #print(Tensor(x))\n",
    "        #print(Tensor(y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y_hat = y_hat.squeeze(1)\n",
    "        #print(y_hat.shape)\n",
    "        #print(y_hat)\n",
    "        loss= F.binary_cross_entropy(y_hat,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(v):\n",
    "    for i in v:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i\n",
    "            \n",
    "\n",
    "STEPS = 100\n",
    "STEP_SIZE = 0.0001\n",
    "\n",
    "\n",
    "def singular_value_decomposition(m):\n",
    "    '''\n",
    "    m = U @ S @ V.T()\n",
    "    \n",
    "    decompose matrix m of dimensions (m,n) into:\n",
    "    U: Unitary matrix of (m,m) i.e. U_inv = U.T() and therefore Det(U) = 1 and therefore cols of U form orthonormal basis of m \n",
    "    S: Rectangular diagonal matrix of (m,n)\n",
    "    V: Unitary matrix of (n,n) i.e. V_inv = V.T() and therefore Det(U) = 1 and therefore cols of V form orthonormal basis of n\n",
    "    \n",
    "    we use gradient descent in order to approximate U, S and V\n",
    "    '''\n",
    "    \n",
    "    # init random approx for U, S, V \n",
    "    U = Array(random=True, dims=(m.dims[0],m.dims[0]), optimisable=True)\n",
    "    \n",
    "    S = Array(random=True, dims=m.dims, optimisable=True).diagonal()\n",
    "    V = Array(random=True, dims=(m.dims[1],m.dims[1]), optimisable=True )\n",
    "    \n",
    "    \n",
    "    # include in loss: loss for non-orthonormality, loss for non-zero off-diagonal elements and finally, for reconstruction approximation, \n",
    "    \n",
    "    def loss_non_orthormality(X):\n",
    "        sim = X.T() @ X\n",
    "        # cols span orthonormal basis: sim ----> off-diagonal elements should be zero\n",
    "        #                  ''              ----> diagonal elements should all be 1 \n",
    "        \n",
    "        sim_diag = sim.diagonal()\n",
    "        idt = sim.identiy()\n",
    "        non_normal = sim_diag - idt \n",
    "        sim_off_diag = sim - sim_diag\n",
    "        \n",
    "        l1 = sum([x**2 for x in list(flatten(sim_off_diag))])\n",
    "        l2 = sum([x**2 for x in list(flatten(non_normal))])\n",
    "        return l1 + l2  \n",
    "    \n",
    "    def loss_off_diagonal(S):\n",
    "        S_diag = S.diagonal()\n",
    "        non_zero = S - S_diag\n",
    "        return sum([x**2 for x in list(flatten(non_zero))])\n",
    "        \n",
    "    \n",
    "    def loss_reconstruction(m, U,S,V):\n",
    "        return sum([x**2 for x in list(flatten(( m - (U @ S @ V.T()) ).values))])\n",
    "        \n",
    "    \n",
    "    for _ in range(STEPS):\n",
    "        \n",
    "        loss = loss_reconstruction(m, U,S,V) + loss_off_diagonal(S) +  loss_non_orthormality(U) + loss_non_orthormality(V)\n",
    "        loss.backwards()\n",
    "        U = U - STEP_SIZE * U.grad()\n",
    "        S = S - STEP_SIZE * S.grad()\n",
    "        V = V - STEP_SIZE * V.grad()\n",
    "        U._zero_grad()\n",
    "        S._zero_grad()\n",
    "        V._zero_grad()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-hQGy9L_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
